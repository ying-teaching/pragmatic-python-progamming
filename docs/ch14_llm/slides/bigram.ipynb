{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bigram Language Model \n",
    "\n",
    "This is based on Andrej's Youtube video [The spelled-out intro to language modeling: building makemore](https://www.youtube.com/watch?v=PaCmpygFfXo).\n",
    "\n",
    "It explains how to develop a simple bigram language model using a neural network. It covers model training, sampling, and the evaluation of a loss function.\n",
    "\n",
    "- [bigram notebook file](https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part1_bigrams.ipynb)\n",
    "- [makemore repository](https://github.com/karpathy/makemore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Dataset and Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
      "32033\n",
      "2 15\n"
     ]
    }
   ],
   "source": [
    "print(words[:10])\n",
    "print(len(words))\n",
    "\n",
    "lengths = [len(word) for word in words]\n",
    "print(min(lengths), max(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Character-Level Language Model\n",
    "\n",
    "A character-level language tries to predict the next character based on the preceding characters. There are `32033` words that each word has a length from `2` to `15`.\n",
    "\n",
    "For example, in the word `isabella`:\n",
    "\n",
    "- `i` is the first character.\n",
    "- each middle character follows another character.\n",
    "- after the last `a`, the word terminates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bigram Language Model\n",
    "\n",
    "A bigram language model works on two characters at a time.\n",
    "\n",
    "It only looks one character and predictes the next character.\n",
    "\n",
    "It is a simple language model for pedagogical purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram From Words\n",
    "\n",
    "Each word generates `n-1` pairs of characters, where `n` is the length of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# each word generates n-1 pairs of characters\n",
    "# where n is the length of the word\n",
    "for w in words[:3]:\n",
    "    for ch1, ch2 in zip(w, w[1:]):\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Start and End\n",
    "\n",
    "However, there are two implict characters in each word: `<S>` and `<E>`.\n",
    "\n",
    "Thus the code to generating bigrams is as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for w in words[:3]:\n",
    "    chars = [\"<S>\"] + list(w) + [\"<E>\"]\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bigram Frequency\n",
    "\n",
    "The prediction is based on the bigram frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "b = {}\n",
    "for w in words:\n",
    "    chars = [\"<S>\"] + list(w) + [\"<E>\"]\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        b[bigram] = b.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('<S>', 'e'), 1531), (('e', 'm'), 769), (('m', 'm'), 168)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('n', '<E>'), 6763), (('a', '<E>'), 6640), (('a', 'n'), 5438)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "three = islice(b.items(), 3)\n",
    "print(list(three))\n",
    "sorted(b.items(), key=lambda item: item[1], reverse=True)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros((3, 5), dtype=torch.int32)\n",
    "a[0, 0] = 5\n",
    "a[1, 3] += 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert characters to integers\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "# print(chars)\n",
    "stoi = {ch: i + 1 for i, ch in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "\n",
    "itos = {i: ch for ch, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27), dtype=torch.int32)\n",
    "\n",
    "for w in words:\n",
    "    chars = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\n",
    "plt.axis('off');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
