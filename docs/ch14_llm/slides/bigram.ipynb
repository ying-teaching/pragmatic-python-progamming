{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bigram Language Model \n",
    "\n",
    "This is based on Andrej's Youtube video [The spelled-out intro to language modeling: building makemore](https://www.youtube.com/watch?v=PaCmpygFfXo).\n",
    "\n",
    "It explains how to develop a simple bigram language model using a neural network. It covers model training, sampling, and the evaluation of a loss function.\n",
    "\n",
    "- [bigram notebook file](https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part1_bigrams.ipynb)\n",
    "- [makemore repository](https://github.com/karpathy/makemore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Dataset and Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(words[:10])\n",
    "print(len(words))\n",
    "\n",
    "lengths = [len(word) for word in words]\n",
    "print(min(lengths), max(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Character-Level Language Model\n",
    "\n",
    "A character-level language tries to predict the next character based on the preceding characters. There are `32033` words that each word has a length from `2` to `15`.\n",
    "\n",
    "For example, in the word `isabella`:\n",
    "\n",
    "- `i` is the first character.\n",
    "- each middle character follows another character.\n",
    "- after the last `a`, the word terminates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bigram Language Model\n",
    "\n",
    "A bigram language model works on two characters at a time.\n",
    "\n",
    "It only looks one character and predictes the next character.\n",
    "\n",
    "It is a simple language model for pedagogical purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram From Words\n",
    "\n",
    "Each word generates `n-1` pairs of characters, where `n` is the length of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# each word generates n-1 pairs of characters\n",
    "# where n is the length of the word\n",
    "for w in words[:3]:\n",
    "    for ch1, ch2 in zip(w, w[1:]):\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Start and End\n",
    "\n",
    "However, there are two implict characters in each word: `<S>` and `<E>`.\n",
    "\n",
    "Thus the code to generating bigrams is as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for w in words[:3]:\n",
    "    chars = [\"<S>\"] + list(w) + [\"<E>\"]\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bigram Frequency\n",
    "\n",
    "The prediction is based on the bigram frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "b = {}\n",
    "for w in words:\n",
    "    chars = [\"<S>\"] + list(w) + [\"<E>\"]\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        b[bigram] = b.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "iterator = iter(b.items())\n",
    "three = [next(iterator) for _ in range(3)]\n",
    "print(list(three))\n",
    "sorted(b.items(), key=lambda item: item[1], reverse=True)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((3, 5), dtype=torch.int32)\n",
    "a[0, 0] = 5\n",
    "a[1, 3] += 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert characters to integers\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "# print(chars)\n",
    "stoi = {ch: i + 1 for i, ch in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "\n",
    "itos = {i: ch for ch, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27), dtype=torch.int32)\n",
    "\n",
    "for w in words:\n",
    "    chars = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each row, create a probability distribution\n",
    "# P = N.float()\n",
    "P = (N + 1).float()\n",
    "P = P / P.sum(dim=1, keepdim=True)  # normalize each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting Semantics\n",
    "\n",
    "The calculation `P = P / P.sum(dim=1, keepdim=True)` is a division between two matrix: `[27, 27] / [27, 1]`. It follows PyTorch [broadcasting semantics](https://pytorch.org/docs/stable/notes/broadcasting.html).\n",
    "\n",
    "- When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is `1`, or one of them does not exist.\n",
    "- If the number of dimensions of `x` and `y` are not equal, prepend `1` to the dimensions of the tensor with fewer dimensions to make them equal length.\n",
    "- Then, for each dimension size, the resulting dimension size is the max of the sizes of `x` and `y` along that dimension.\n",
    "\n",
    "In the above, the two `broadcastable` based on the above rules. The column vector `[27, 1]` is extended by copying the first column `27` times to match the size of the first matrix. Then it does an element-wise division."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting Can Cause Bug\n",
    "\n",
    "In the above code, if the second argument of  `P.sum(dim=1, keepdim=True)` is missing, the result is a `[27]` vector. In division, it is treated as `[1, 27]`. the values will be replicated `27` times for each row to become `[27, 27]`.\n",
    "\n",
    "The correct one should replicate the `27` sum values for each column. Use a three value `(10, 20, 30)` example, each value is the sum of each row.\n",
    "\n",
    "- Missing `keepdim=True`, the shape is `[27]` and values are replicated by rows (vertically): `[[10, 20, 30], [10, 20, 30], [10, 20, 30]]`.\n",
    "- With `keepdim=True`, the shape is `[27, 1]` and  values are replicated by column (horizontally): `[[10, 10, 10], [20, 20, 20], [30, 30, 30]]`.\n",
    "\n",
    "Because each value `(10, 20, 30)` in is the sum of each row, every element in the first row should be divided by `10`, every element in the second row should be divided by `20`, an so on so forth. Therefore, `keepdim=True` gives the correct result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use generator to ensure reproducibility in the same computer !\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for _ in range(10):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        ix = torch.multinomial(P[ix], 1, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model Quality\n",
    "\n",
    "Using [Maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation), the likelihood is the product of probabilities of each pair. The likelihood of the entire data set is the measured quality of the training model. Higher is better.\n",
    "\n",
    "The real probability for each pair is typically small, for example, the average should be `1/27 = 0.037`.  Thus, `logprob` is used. The range of `logprob` is from `-infinity` (when `p = 0`) to `0` (when `p=1`). The likelihood for `logprob` is the sum of the `logprob`.\n",
    "\n",
    "To make it a loss function (smaller is btter), use `nll` (negative log likelihood) that is the average negative log likilihood of each pair. The lowest `nll` value is `0`.\n",
    "\n",
    "Thus. to maximize the likelihood is to minimize the `nll`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "log_likelihood = 0\n",
    "\n",
    "# for w in [\"andrej\"]:\n",
    "\n",
    "# # this \"jq\" pair is not in the dataset, so the log likelihood should be -inf, need smoothing. add one to every possible pair.\n",
    "# for w in [\n",
    "#     \"andrejq\"\n",
    "# ]:\n",
    "\n",
    "for w in words[:]:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        # print(f\"{ch1}{ch2}: {prob:.4f}, {logprob:.4f}\")\n",
    "\n",
    "print(f\"log likelihood: {log_likelihood:.4f}\")\n",
    "nll = -log_likelihood / n\n",
    "print(f\"nll: {nll:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So Far\n",
    "\n",
    "The above code calculates normalized probability of every pair.\n",
    "\n",
    "The pair probability can be used to generate new words using multinomial distribution.\n",
    "\n",
    "`nll` is developed as the loss funciton to measure the model quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Approach\n",
    "\n",
    "The NN model receives a single character as an input and create an output that is a probability distribution over the next character.\n",
    "\n",
    "The `nll` is used to tune the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training set of all bigrams (x, y).\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    chars = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)  # becarful the data type\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "Neurons perform calculations on inputs with weights and biases. It doesn't make sense to use the character index as an input value.\n",
    "\n",
    "PyTorch has `One Hot` encoding that creates vector for each index value, only the index poisition is `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()  # need to convert to float\n",
    "print(xenc)\n",
    "print(xenc.shape)\n",
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 27 different characters - that's the `num_classes` value is.\n",
    "Each input is one of the 27 characters, the index position is 1, other positions are 0.\n",
    "A single character in the data set is mapped into a tenor of the shape of `[27]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((27, 1))  # 1 neuron\n",
    "print(W)\n",
    "xenc @ W  # five inputs, one neuron, resul is 5x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neuron takes 27-dimensional inputs , corresponding to the `num_classes` of the one-hot input. Each character input is represented by a float tensor of shape `[27]`.\n",
    "\n",
    "The `W` tensor is a NN layer where the first value is the dimension number of the input, the second is the number of neurons in the layer.\n",
    "\n",
    "The neuron's weights are initilialized from standard normal distribution. Most values (about `68%`) are between `[-1, 1]`, few (about `0.3%`) is smaller than `-3` or larger than `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((27, 27))  # 27 neurons\n",
    "logits = xenc @ W  # five inputs, 27 neurons, result is [5, 27]\n",
    "\n",
    "# matrix multiplication is the sum of dot product\n",
    "print(logits[3, 13])\n",
    "print((xenc[3] * W[:, 13]).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of `xenc @ W` can be interpreted as the \"firing rates\" of neurons for the training samples. \n",
    "\n",
    "It is important to accurately interprete the 27 output values of the 27 neurons. Their initial values are **randomly** generated from standard normal distribution.\n",
    "\n",
    "Intuitively, for each input, the NN predicts the probability distribution for the next character. There are 27 possible next characters. The sum of the probality is 1. \n",
    "\n",
    "So the goal is to transform the 27 output values into probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "Semantically, the output values of the one layer NN can be interpreted as a result of `log(counts)` - called count `logits`.\n",
    "\n",
    "To get the counts, exponentiate the logit values. All values below `0` will be smaller than `1`. All positive numbers are transformed into values bigger than `1`. `logits.exp()` is similar to the `N` count matrix in the previous probability model.\n",
    "\n",
    "Once we have the counts, probability distribution is easy. \n",
    "\n",
    "The whole process is a `softmax` function that \"squashes\" a K-dimensional float values to a K-dimensional vector of float values in the range `(0, 1)` that add up to 1 - the key feature of probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "probs[3].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NN Model\n",
    "\n",
    "- The input is `one-hot` encoded as a 27-dimensional data.\n",
    "- Each neuron has a 27 inputs.\n",
    "- A single layer has 27 neurons therefore it has 27 outputs.\n",
    "- The neuron transformation is differentiable, therefore it supports backpropagation to update gradients and tune weights.\n",
    "    - only linear transformation (even no bias)\n",
    "    - the `softmax` function is also differentiable \n",
    "- The 27 outputs are interpreted the 27 probabilities for the 27 characters to be the next character for a given character.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summary\n",
    "\n",
    "- `xs` are inputs, `ys` are labels (the actual next characters).\n",
    "- `g` is a generator that uses a manual seed thus the process is repeatable.\n",
    "- `W` is an one-layer NN that have 27 neurons, each neuron receives 27 inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    chars = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)  # becarful the data type\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g)  # 27 neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foward Pass\n",
    "\n",
    "The forward pass is differentiable: every operation can be back-backpropagated.\n",
    "\n",
    "- predict the log-counts\n",
    "- `softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()  # encode the input, convert to float\n",
    "logits = xenc @ W  # predict the log-counts\n",
    "\n",
    "# the softmax function\n",
    "counts = logits.exp()  # counts, equivalent to N\n",
    "probs = counts / counts.sum(dim=1, keepdim=True)  # probabilities, equivalent to P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Loss Function\n",
    "\n",
    "For each input `i`,  `x = xs[i]` is the input character index, there is a corresponding label `y = ys[i]` where `y` is the index of the correct character.\n",
    "\n",
    "Therefore, `probs[i, y]` is the predicted probability for `x`. Of course, `1` is a perfect probability value for `probs[i, y]`.\n",
    "\n",
    "The loss function is defined as the mean of negative log likelihood of probabilities for all inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram: .e, index: 0, 5\n",
      "prob: 0.0083, nll: 4.7944\n",
      "bigram: em, index: 5, 13\n",
      "prob: 0.0033, nll: 5.7115\n",
      "bigram: mm, index: 13, 13\n",
      "prob: 0.0181, nll: 4.0130\n",
      "bigram: ma, index: 13, 1\n",
      "prob: 0.0105, nll: 4.5541\n",
      "bigram: a., index: 1, 0\n",
      "prob: 0.0866, nll: 2.4464\n",
      "average negative log likelihood, i.e. loss =  4.3039\n"
     ]
    }
   ],
   "source": [
    "num_inputs = len(xs)\n",
    "nlls = torch.zeros(num_inputs)\n",
    "\n",
    "for i in range(num_inputs):\n",
    "    x = xs[i].item()\n",
    "    y = ys[i].item()\n",
    "    print(f\"bigram: {itos[x]}{itos[y]}, index: {x}, {y}\")\n",
    "    nlls[i] = -torch.log(probs[i, y])\n",
    "    print(f\"prob: {probs[i, y]:.4f}, nll: {nlls[i]:.4f}\")\n",
    "\n",
    "print(f\"average negative log likelihood, i.e. loss =  {nlls.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Loss Values\n",
    "\n",
    "Different random intialization parameters generate different loss values.\n",
    "\n",
    "The right approach is to tune `W` using gradient-based optimization - a typical NN training process. Repeat the following loop for a number of time; or the loss value is small enough; or there is no significant improvement.\n",
    "\n",
    "- run forward pass\n",
    "  - evaluate input\n",
    "  - get loss value\n",
    "- run backward pass\n",
    "    - reset parameters\n",
    "    - run `loss.backward()`\n",
    "- update parameters\n",
    "    - tune parameters with gradients and step size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Advantages\n",
    "\n",
    "The NN result is almost same as the best probability model - it proves its validity.\n",
    "\n",
    "Scalable: more characters. \n",
    "\n",
    "Only change the neuron computation\n",
    "\n",
    "Insights:\n",
    "\n",
    "- In `xenc@W`, one hot encoding picks the specific row. `W` is log count, not raw count.\n",
    "- Smoothing, increasing count smoot the probability. `W` is initialized randomly. If setting to 0, it smooths. can be used for regularization, used in loss funciton will push `W` to be 0. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
