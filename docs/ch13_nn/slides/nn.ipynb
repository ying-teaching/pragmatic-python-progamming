{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Neural Network\n",
    "\n",
    "It is based on Andrej's fantastic Youtube video [The spelled-out intro to neural networks and backpropagation: building `micrograd`](https://www.youtube.com/watch?v=VMj-3S1tku0).\n",
    "\n",
    "It explains how deep neural network (DNN) works under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The library implements back-propagation algorithm that is used to efficiently \n",
    "\n",
    "- evaluate the gradient of \n",
    "- a loss function with respect to \n",
    "- the weights of \n",
    "- a neural network.\n",
    "\n",
    "It improves the NN accuracy by iteratively tuning the weights of the neural network to minize the loss function.\n",
    "\n",
    "Back-propagation is the mathematical core of a DNN library like `PyTorch`.\n",
    "\n",
    "Let's see a simple lib named [Micrograd](https://github.com/karpathy/micrograd)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Back-propagation\n",
    "\n",
    "A calculation can be expressed as a series of expressions. For example: \n",
    "\n",
    "```python\n",
    "c = a + b\n",
    "d = a + b * b ** 3\n",
    "e = c - d\n",
    "g = e ** 2\n",
    "```\n",
    "\n",
    "`g.backward()` is the back-propagation. It starts at `g`, goes backward through its expression graph and recursively applies the calculus chain rule to evaluate the partial derivative of `g` with respect to the all internal nodes (`e`, `d`, and `c`) and inputs (`a` and `b`). The derivatives are stores in the `.grad` attribute of each value.\n",
    "\n",
    "A derivative tells how a value affects the expression result. It is the slope of growth of the value with regard to the result.\n",
    "\n",
    "Though this made-up expression doesn't mean anything, it shows how deep neural network works. DNN are just mathematic expressions like this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DNN\n",
    "\n",
    "- A DNN is just a certain type of mathematic expression.\n",
    "- The inputs for a DNN include input values and weights of DNN.\n",
    "- The output of a DNN is prediction or a loss function.\n",
    "\n",
    "For pedagogical reasons, Micrograd works on scalar values. A typical DNN works on multi-dimension data (called `tensor`s in PyTorch) for effecient parallel computation. They are fundamentally the same thing.\n",
    "\n",
    "Micrograd has an engine `engine.py` that has less than 100 lines of Python code. The `nn.py` defines a DNN consists of `Neuron`, `Layer` and `MLP`. It only has 60 lines of Python code. \n",
    "\n",
    "That's enough to understand how DNN training works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Engine Code\n",
    "\n",
    "To run the demo code, please install Python libraries\n",
    "\n",
    "```sh\n",
    "pip install jupyter # for Jupyter Notebook\n",
    "pip install numpy matplotlib pandas seaborn statsmodels # for computation and visualization\n",
    "pip install graphviz # build expression graph\n",
    "```\n",
    "\n",
    "You also need to install `graphviz` application in your OS.\n",
    "In MacOS, it might be `brew install graphviz`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Value:\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "a = Value(2.0, label=\"a\")\n",
    "b = Value(-3.0, label=\"b\")\n",
    "c = Value(10.0, label=\"c\")\n",
    "d = a * b\n",
    "d.label = \"d\"\n",
    "L = c + d\n",
    "L.label = \"L\"\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "\n",
    "def trace(root):\n",
    "    # builds a set of all nodes and edges in a graph\n",
    "    nodes, edges = set(), set()\n",
    "\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format=\"svg\", graph_attr={\"rankdir\": \"LR\"})  # LR = left to right\n",
    "\n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        # for any value in the graph, create a rectangular ('record') node for it\n",
    "        dot.node(\n",
    "            name=uid,\n",
    "            label=\"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad),\n",
    "            shape=\"record\",\n",
    "        )\n",
    "        if n._op:\n",
    "            # if this value is a result of some operation, create an op node for it\n",
    "            dot.node(name=uid + n._op, label=n._op)\n",
    "            # and connect this node to it\n",
    "            dot.edge(uid + n._op, uid)\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        # connect n1 to the op node of n2\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Derivative Chain Rule\n",
    "\n",
    "![img](../images/chain-rule.jpg)\n",
    "\n",
    "The derivative of the composite function is the inner function ‍ within the derivative of the outer function, multiplied by the derivative of the inner function.\n",
    "\n",
    "A [Wikipedia example](https://en.wikipedia.org/wiki/Chain_rule): \n",
    "> If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 × 4 = 8 times as fast as the man.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def backward(self):\n",
    "\n",
    "    topo = []\n",
    "    visited = set()\n",
    "\n",
    "    def build_topo(v):\n",
    "        if v not in visited:\n",
    "            visited.add(v)\n",
    "            for child in v._prev:\n",
    "                build_topo(child)\n",
    "            topo.append(v)\n",
    "\n",
    "    build_topo(self)\n",
    "\n",
    "    self.grad = 1.0\n",
    "    for node in reversed(topo):\n",
    "        node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def __add__(self, other):\n",
    "    out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "    def _backward():\n",
    "        self.grad += 1.0 * out.grad\n",
    "        other.grad += 1.0 * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def __mul__(self, other):\n",
    "    # to allow for scalar multiplication\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "    def _backward():\n",
    "        self.grad += other.data * out.grad\n",
    "        other.grad += self.data * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A Neuron\n",
    "\n",
    "A neuron is an expression with several inputs and one bias.\n",
    "It creates one output.\n",
    "\n",
    "For example: a partial neuron with two inputs can be expressed as \n",
    "\n",
    "`output = input_1 * weight_1 + input_2 * weight_2 + bias`\n",
    "\n",
    "Weights and bias are **parameters**. \n",
    "\n",
    "It is partial because it misses a non-linear transformation - described later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# a neuron with two inputs and one output\n",
    "# inputs x1,x2\n",
    "x1 = Value(2.0, label=\"x1\")\n",
    "x2 = Value(1.5, label=\"x2\")\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0, label=\"w1\")\n",
    "w2 = Value(1.0, label=\"w2\")\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label=\"b\")\n",
    "\n",
    "# x1w1 = x1 * w1\n",
    "# x2w2 = x2 * w2\n",
    "# ws = x1w1 + x2w2\n",
    "# o = ws + b\n",
    "\n",
    "o = (x1 * w1 + x2 * w2) + b\n",
    "\n",
    "print(f\"output: {o.data:.4f}\")\n",
    "o.backward()\n",
    "\n",
    "print(\"---\")\n",
    "print(\"w1 grad\", w1.grad)\n",
    "print(\"w2 grad\", w2.grad)\n",
    "print(\"b grad\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Computation Goal\n",
    "\n",
    "If the computation goal is a loss function value that we want to minimize.\n",
    "If the goal is to maximize a value, just change the adjustment direction,or negate the goal value to make it a minimization goal.\n",
    "\n",
    "With `step_size` (learning rate), adjust parameters (weights and biases) because the inputs are given.\n",
    "\n",
    "- Decrease a value's weight if its gradient is positive:\n",
    "  - `v.weight -= step_size * v.gradient`\n",
    "- Increase a value's weight if its gradient is negative:\n",
    "  - `v.weight -= step_size * v.gradient`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "step_rate = 0.01\n",
    "w1.data -= step_rate * w1.grad\n",
    "w2.data -= step_rate * w2.grad\n",
    "b.data -= step_rate * b.grad\n",
    "\n",
    "o = x1 * w1 + x2 * w2 + b\n",
    "\n",
    "print(f\"output: {o.data:.4f}\")\n",
    "print(\"---\")\n",
    "print(\"w1\", w1)\n",
    "print(\"w2\", w2)\n",
    "print(\"b\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The PyTorch Library\n",
    "\n",
    "PyTorch is a framework for building deep learning models.\n",
    "\n",
    "The `Value` class uses the PyTorch API for its data element.\n",
    "\n",
    "`pip3 install torch torchvision`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.Tensor([2.0]).double()\n",
    "x1.requires_grad = True\n",
    "x2 = torch.Tensor([1.5]).double()\n",
    "x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double()\n",
    "w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()\n",
    "w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double()\n",
    "b.requires_grad = True\n",
    "n = x1 * w1 + x2 * w2 + b\n",
    "\n",
    "print(n.data.item())\n",
    "n.backward()\n",
    "\n",
    "print(\"---\")\n",
    "print(\"x2\", x2.grad.item())\n",
    "print(\"w2\", w2.grad.item())\n",
    "print(\"x1\", x1.grad.item())\n",
    "print(\"w1\", w1.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLP: Multi-Layer Perceptron\n",
    "\n",
    "![img](../images/mlp.png)\n",
    "\n",
    "Source: https://www.pycodemates.com/2023/01/multi-layer-perceptron-a-complete-overview.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Weights and Bias\n",
    "\n",
    "![img](../images/weights-and-bias.png)\n",
    "\n",
    "![img](../images/formula.png)\n",
    "\n",
    "Source: https://www.pycodemates.com/2023/01/multi-layer-perceptron-a-complete-overview.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Neuron (Finally)\n",
    "\n",
    "The design of Neuron follows the PyTorch API based on a popular conceptual DNN model.\n",
    "\n",
    "- A neuron has a number of inputs and a bias.\n",
    "    - Each input `x` has a weight `w`. \n",
    "    - The input weights and the bias are **parameters**. \n",
    "    - All are initialized with random numbers.\n",
    "- A neuron has one output that \n",
    "    - First, add the `bias` and the weighted sum of input: the `dot` product of `W` (all weights) and `X` (all inputs). The result is `z = WX + bias`.\n",
    "    - Second, the sum is transformed by an activation function like `tanh` or `relu` (Rectified Linear Unit) to make it can handle complex non-linear relationships. Otherwise, the result is a simple linear transformation that is far from enough for real world knowledge.\n",
    "    - The final output value is `tanh(WX + bias)` or `relu(WX + bias)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Activation Function\n",
    "\n",
    "In NN, activation function is used to transform input data to a desired one. Common activation functions include `Sigmoid`, `tanh` and `relu`. `relu` becomes popular due to its computational efficiency.\n",
    "\n",
    "![img](../images/tanh.webp)\n",
    "\n",
    "Source: https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  `relu`\n",
    "\n",
    "```python\n",
    "# method of class Value\n",
    "def relu(self):\n",
    "    out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "    def _backward():\n",
    "        self.grad += (out.data > 0) * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    return out\n",
    "```\n",
    "\n",
    "![img](../images/relu.png)\n",
    "Source: https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Value:\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(\n",
    "            other, (int, float)\n",
    "        ), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f\"**{other}\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):  # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):  # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __neg__(self):  # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):  # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __radd__(self, other):  # other + self\n",
    "        return self + other\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2 * x) - 1) / (math.exp(2 * x) + 1)\n",
    "        out = Value(t, (self,), \"tanh\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), \"exp\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (\n",
    "                out.data * out.grad\n",
    "            )  # NOTE: in the video I incorrectly used = instead of +=. Fixed here.\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "\n",
    "    # forward calculation of the neuron\n",
    "    def __call__(self, x):\n",
    "        # w * x + b. sum takes a start value. We start with b\n",
    "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = [2.0, 3.0]\n",
    "neuron = Neuron(2)\n",
    "print(neuron.parameters())\n",
    "out = neuron(x)\n",
    "print(out.data)\n",
    "out.backward()\n",
    "print(neuron.w[0].grad, neuron.w[1].grad, neuron.b.grad)\n",
    "print(len(neuron.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Layer\n",
    "\n",
    "- A `Layer` has a number of `Neuron`s, each `Neuron` is fully connected to all inputs.\n",
    "- Each `Neuron` evaluates its inputs independently.\n",
    "- It has a number of output is the number of neurons because each neuron has one output.\n",
    "\n",
    "How many parameters do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = [2.0, 3.0]\n",
    "layer = Layer(2, 3)\n",
    "out = layer(x)\n",
    "print(out)\n",
    "print(len(layer.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MLP\n",
    "\n",
    "A MLP has many layers that whose inputs and outputs are connected between two layers.\n",
    "\n",
    "How many parameters do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    # nin is the number of input neurons\n",
    "    # nouts is a list of number of neurons in each layer\n",
    "    def __init__(self, nin, nouts):\n",
    "        layer_sizes = [nin] + nouts\n",
    "        layer_range = range(len(nouts))\n",
    "        self.layers = [Layer(layer_sizes[i], layer_sizes[i + 1]) for i in layer_range]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4, 4, 1])\n",
    "o = n(x)\n",
    "print(o.data)\n",
    "print(len(n.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Training\n",
    "\n",
    "A NN is initialized with random parameters (weights and biases) when it is created.\n",
    "Then feed it with inputs and desired targets, based on the difference (a loss value) between its outputs (predicted values) and the desired targets, tune its parameters to minimize the loss value.\n",
    "\n",
    "For example, a batch usually have multiple inputs, first run forward pass:\n",
    "\n",
    "- for input `[2, 3, -1.0]`, the output should be `1.0` (might mean `YES`/`On`/`High`...)\n",
    "- for input `[3, -1, 0.5]`, the output should be `-1.0` (might mean `No`/`Off`/`Low`...)\n",
    "- ... \n",
    "\n",
    "Then, calculate the total loss of each batch, ran `loss.backward()` (the backward pass) to tune the parameters to minimize loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n = MLP(3, [4, 4, 1])\n",
    "\n",
    "# forward pass\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]  # desired targets\n",
    "\n",
    "ypred = [n(x) for x in xs]\n",
    "print(ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tuning Parameter\n",
    "\n",
    "Define a `loss` function to measure the difference between predicted and the target. It is the optimization goal that is to be minimized. There are many loss functions.\n",
    "\n",
    "Staring from the `loss`, use back-propagation to calculate gradients.\n",
    "\n",
    "Don't forget resetting gradients to zero !! But why?\n",
    "\n",
    "Then adjust parameters to minimize the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for k in range(20):\n",
    "\n",
    "    # forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum((yout - ygt) ** 2 for ygt, yout in zip(ys, ypred))\n",
    "\n",
    "    # backward pass, don't forget resetting gradients to zero !!\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in n.parameters():\n",
    "        p.data -= 0.1 * p.grad\n",
    "\n",
    "    print(k, loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- A neural network is a set of mathematical expression - represented as MLP.\n",
    "- It takes inputs and has weights/bias as its parameters.\n",
    "- Repeat the following steps until the prediction is good enough.\n",
    "    - The forward pass calculates prediction values.\n",
    "    - The loss function measures the accuracy of the prediction. The lower the loss value, the better the prediction result.\n",
    "    - The backward (back-propagation) of the loss calculates the gradients of its neurons.\n",
    "    - Adjust parameters based on their gradients to decrease the loss value. The step size (learning rate) determines the amount the changes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
